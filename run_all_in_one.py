# -*- coding: utf-8 -*-
# í†µí•© ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ v5: run_all_in_one.py
# ê¸°ëŠ¥: ë™ì  í¬ë¡¤ë§, ì¤‘ë³µ ì œê±°, ìŠ¤í¬ë© ê°€ì¹˜ ì˜ˆì¸¡, ì •ì  íŒŒì¼ ìƒì„±ì„ ëª¨ë‘ ìˆ˜í–‰í•©ë‹ˆë‹¤.

import pandas as pd
import numpy as np
import re
import pickle
import os
import requests
from bs4 import BeautifulSoup
from konlpy.tag import Okt
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.sparse import hstack
import time
import json
import sys
import io
from tqdm import tqdm
from sklearn.metrics.pairwise import cosine_similarity

# --- ì „ì—­ ì„¤ì • ë° íŒŒì¼ ê²½ë¡œ ---

# SSL ì˜¤ë¥˜ ë°©ì§€ ì„¤ì • (í•„ìš” ì‹œ)
os.environ['CURL_CA_BUNDLE'] = ''
os.environ['REQUESTS_CA_BUNDLE'] = ''
os.environ['HF_HUB_DISABLE_CERTIFICATE_VERIFICATION'] = '1'

# íŒŒì¼ ê²½ë¡œ ì •ì˜ (ìƒëŒ€ ê²½ë¡œ ì‚¬ìš©)
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
SCRAP_DIR = os.path.join(BASE_DIR, 'scrap')
SCRAPPED_NEWS_TODAY_CSV = os.path.join(SCRAP_DIR, 'scrapped_news_today.csv')
MODEL_PATH = os.path.join(SCRAP_DIR, 'scrap_model.pkl')
VECTORIZER_PATH = os.path.join(SCRAP_DIR, 'tfidf_vectorizer.pkl')
JSON_OUTPUT_FILE = os.path.join(BASE_DIR, 'news_data.json')
HTML_TEMPLATE_FILE = os.path.join(SCRAP_DIR, 'templates', 'index.html')
HTML_OUTPUT_FILE = os.path.join(BASE_DIR, 'index.html')
JS_TEMPLATE_FILE = os.path.join(SCRAP_DIR, 'static', 'js', 'main.js')
JS_OUTPUT_FILE = os.path.join(BASE_DIR, 'static', 'js', 'github_pages_main.js')

# --- ê³µí†µ ì „ì²˜ë¦¬ í•¨ìˆ˜ ë° ê°ì²´ ---
okt = Okt()
def preprocess(text):
    """ í…ìŠ¤íŠ¸ì—ì„œ ëª…ì‚¬ë§Œ ì¶”ì¶œí•˜ì—¬ ê³µë°±ìœ¼ë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤. """
    return ' '.join(okt.nouns(re.sub(r'[^\ã„±-ã…ã…-ã…£ê°€-í£ ]','', str(text))))

# --- 1ë‹¨ê³„: ë™ì  í¬ë¡¤ë§, ì¤‘ë³µ ì œê±° ë° ì „ì²˜ë¦¬ ---

def get_today_articles():
    """ 5ëŒ€ ì¼ê°„ì§€ì—ì„œ ì˜¤ëŠ˜ì˜ ì£¼ìš” ê¸°ì‚¬ ëª©ë¡(ì œëª©, ë§í¬, ì‹ ë¬¸ì‚¬)ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤. """
    print("--- 1.1: ì£¼ìš” ì¼ê°„ì§€ ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘ ì‹œì‘ ---")
    newspapers = {
        'í•œêµ­ê²½ì œ': '015', 'ë§¤ì¼ê²½ì œ': '009', 'ë™ì•„ì¼ë³´': '020',
        'ì¡°ì„ ì¼ë³´': '023', 'ì¤‘ì•™ì¼ë³´': '025',
    }
    headers = {'User-Agent': 'Mozilla/5.0'}
    all_articles = []
    for name, oid in newspapers.items():
        try:
            url = f"https://media.naver.com/press/{oid}/newspaper"
            response = requests.get(url, headers=headers, verify=False, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            links = soup.select('div.sc_offc_lst._paper_article_list a')
            for link in links:
                title = link.get_text(strip=True)
                href = link.get('href', '')
                if title and href:
                    all_articles.append({
                        'ì‹ ë¬¸ì‚¬': name, 'ì œëª©': title,
                        'ë§í¬': 'https://media.naver.com' + href if href.startswith('/') else href
                    })
            time.sleep(0.5)
        except Exception as e:
            print(f"  âœ— {name} ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
    df = pd.DataFrame(all_articles).drop_duplicates(subset=['ë§í¬']).reset_index(drop=True)
    print(f"  - ì´ {len(df)}ê°œì˜ ê³ ìœ  ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘ ì™„ë£Œ.")
    return df

def get_article_content(url):
    """ ë„¤ì´ë²„ ë‰´ìŠ¤ ë§í¬ì—ì„œ ë³¸ë¬¸ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤. """
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=10, verify=False)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        content_area = soup.select_one('#dic_area, #articeBody, #article_content')
        if content_area:
            for el in content_area.select('script, style, .reporter_area, .ad_area, .promotion_area, div.byline, a, span.end_photo_org'):
                el.decompose()
            return content_area.get_text(strip=True)
    except Exception as e:
        print(f"    - ë§í¬ í¬ë¡¤ë§ ì‹¤íŒ¨: {url}, ì˜¤ë¥˜: {e}")
    return ""

def run_step_one_crawling_and_preprocessing():
    """ í¬ë¡¤ë§, ìš”ì•½, ì¤‘ë³µ ì œê±°, ì „ì²˜ë¦¬ë¥¼ í†µí•© ì‹¤í–‰í•©ë‹ˆë‹¤. """
    articles_df = get_today_articles()
    if articles_df.empty: return pd.DataFrame()

    print("--- 1.2: ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§ ë° ìš”ì•½ ìƒì„± ---")
    crawled_data = []
    for _, row in tqdm(articles_df.iterrows(), total=len(articles_df), desc="  - ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§"):
        content = get_article_content(row['ë§í¬'])
        if content:
            crawled_data.append([
                row['ì‹ ë¬¸ì‚¬'], row['ì œëª©'], row['ë§í¬'], content, content[:400] + "..."
            ])
        time.sleep(0.5)
    
    if not crawled_data:
        print("í¬ë¡¤ë§ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame()

    df = pd.DataFrame(crawled_data, columns=['ì‹ ë¬¸ì‚¬', 'ì œëª©', 'ë§í¬', 'ë³¸ë¬¸', 'ë³¸ë¬¸_ìš”ì•½'])

    print("--- 1.3: ë‚´ìš© ê¸°ë°˜ ì¤‘ë³µ ê¸°ì‚¬ ì œê±° (ìœ ì‚¬ë„ 0.6 ê¸°ì¤€) ---")
    if not df.empty and 'ë³¸ë¬¸' in df.columns and not df['ë³¸ë¬¸'].isnull().all():
        df['processed_text_for_dedup'] = df['ë³¸ë¬¸'].apply(preprocess)
        vectorizer = TfidfVectorizer(min_df=1, ngram_range=(1, 2))
        tfidf_matrix = vectorizer.fit_transform(df['processed_text_for_dedup'])
        cosine_sim = cosine_similarity(tfidf_matrix)
        
        df['ë³¸ë¬¸_ê¸¸ì´_dedup'] = df['ë³¸ë¬¸'].str.len()
        df = df.sort_values(by='ë³¸ë¬¸_ê¸¸ì´_dedup', ascending=False).reset_index(drop=True)
        
        tfidf_matrix_sorted = vectorizer.fit_transform(df['processed_text_for_dedup'])
        cosine_sim_sorted = cosine_similarity(tfidf_matrix_sorted)
        
        to_drop = set()
        for i in range(len(cosine_sim_sorted)):
            if i in to_drop: continue
            for j in range(i + 1, len(cosine_sim_sorted)):
                if j in to_drop: continue
                if cosine_sim_sorted[i, j] > 0.6:
                    to_drop.add(j)
        
        if to_drop:
            print(f"   - {len(to_drop)}ê°œì˜ ìœ ì‚¬ ê¸°ì‚¬ë¥¼ ì œê±°í–ˆìŠµë‹ˆë‹¤.")
            df.drop(index=list(to_drop), inplace=True)
        
        df.drop(columns=['processed_text_for_dedup', 'ë³¸ë¬¸_ê¸¸ì´_dedup'], inplace=True)

    print(f"   - ìµœì¢… ë¶„ì„ ëŒ€ìƒ ê¸°ì‚¬: {len(df)}ê°œ")
    print("--- 1ë‹¨ê³„: í¬ë¡¤ë§ ë° ì „ì²˜ë¦¬ ì™„ë£Œ ---")
    print()
    return df

# --- 2ë‹¨ê³„: ìŠ¤í¬ë© ê°€ì¹˜ ì˜ˆì¸¡ ---

def run_step_two_prediction(df):
    """ í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìŠ¤í¬ë© ê°€ì¹˜ë¥¼ ì˜ˆì¸¡í•˜ê³  ìµœì¢… ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥í•©ë‹ˆë‹¤. """
    if df.empty:
        print("2ë‹¨ê³„ ì‹¤íŒ¨: ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return False
        
    print("--- 2.1: ëª¨ë¸ ë° Vectorizer ë¡œë“œ ---")
    try:
        with open(MODEL_PATH, "rb") as f: model = pickle.load(f)
        with open(VECTORIZER_PATH, "rb") as f: tfidf_vectorizer = pickle.load(f)
    except FileNotFoundError:
        print(f"ì˜¤ë¥˜: ëª¨ë¸ íŒŒì¼({MODEL_PATH}) ë˜ëŠ” Vectorizer íŒŒì¼({VECTORIZER_PATH})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return False

    print("--- 2.2: í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° íŠ¹ì„± ì¶”ì¶œ ---")
    keywords = {
        'ì—…ê³„': ['ì‹í’ˆ', 'í™”í•™', 'ë°”ì´ì˜¤', 'íŒ¨í‚¤ì§•', 'í”Œë¼ìŠ¤í‹±', 'í•­ì•”ì œ', 'ë°°í„°ë¦¬', 'ì¹œí™˜ê²½', 'Dë¨', 'ì‚¼ì–‘', 'ì´ˆìˆœìˆ˜', 'ì œì•½ì‚¬', 'ìˆ™ì·¨', 'ìƒì¾Œí™˜', 'ì„¤íƒ•', 'ì¹¼ë¡œë¦¬', 'ì‚¼ì–‘ê·¸ë£¹', 'ì‚¼ì–‘ì‚¬', 'ì‚¼ì–‘íŒ¨í‚¤ì§•', 'ì‚¼ì–‘ì—”ì”¨ì¼', 'ì‚¼ì–‘ë°”ì´ì˜¤íŒœ'],
        'ê²½ì˜': ['ê²½ì˜', 'ê²½ì œ', 'í™˜ìœ¨', 'M&A', 'ì¸ìˆ˜', 'íˆ¬ì', 'ì‹¤ì ', 'í•œì¼ê²½ì œí˜‘íšŒ', 'ì¡°ì§ë¬¸í™”', 'ë¬´ì—­']
    }

    df['processed_title'] = df['ì œëª©'].apply(preprocess)
    df['processed_text'] = df.apply(lambda r: preprocess(r['ì œëª©'] + " " + r['ë³¸ë¬¸']), axis=1)
    
    df['ì—…ê³„_í‚¤ì›Œë“œ_ê°œìˆ˜'] = df['processed_text'].apply(lambda x: sum(k in x for k in keywords['ì—…ê³„']))
    df['ê²½ì˜_í‚¤ì›Œë“œ_ê°œìˆ˜'] = df['processed_text'].apply(lambda x: sum(k in x for k in keywords['ê²½ì˜']))
    df['ì—…ê³„_í‚¤ì›Œë“œ_ì œëª©_ê°œìˆ˜'] = df['processed_title'].apply(lambda x: sum(k in x for k in keywords['ì—…ê³„']))
    df['ê²½ì˜_í‚¤ì›Œë“œ_ì œëª©_ê°œìˆ˜'] = df['processed_title'].apply(lambda x: sum(k in x for k in keywords['ê²½ì˜']))
    df['ë³¸ë¬¸_ê¸¸ì´'] = df['ë³¸ë¬¸'].str.len().replace(0, 1)
    df['ì—…ê³„_í‚¤ì›Œë“œ_ë°€ë„'] = df['ì—…ê³„_í‚¤ì›Œë“œ_ê°œìˆ˜'] / df['ë³¸ë¬¸_ê¸¸ì´']
    df['ê²½ì˜_í‚¤ì›Œë“œ_ë°€ë„'] = df['ê²½ì˜_í‚¤ì›Œë“œ_ê°œìˆ˜'] / df['ë³¸ë¬¸_ê¸¸ì´']

    text_features = tfidf_vectorizer.transform(df['processed_text'])
    metadata_features = df[['ì—…ê³„_í‚¤ì›Œë“œ_ê°œìˆ˜', 'ê²½ì˜_í‚¤ì›Œë“œ_ê°œìˆ˜', 'ì—…ê³„_í‚¤ì›Œë“œ_ì œëª©_ê°œìˆ˜', 'ê²½ì˜_í‚¤ì›Œë“œ_ì œëª©_ê°œìˆ˜', 'ë³¸ë¬¸_ê¸¸ì´', 'ì—…ê³„_í‚¤ì›Œë“œ_ë°€ë„', 'ê²½ì˜_í‚¤ì›Œë“œ_ë°€ë„']].values
    
    X_new = hstack([text_features, metadata_features * 0.5]).tocsr()

    print("--- 2.3: ìŠ¤í¬ë© ê°€ì¹˜ ì ìˆ˜ ì˜ˆì¸¡ ë° ê²°ê³¼ ì €ì¥ ---")
    df['ì˜ˆì¸¡ì ìˆ˜'] = model.predict_proba(X_new)[:, 1]
    
    df['ì¹´í…Œê³ ë¦¬'] = 'ê¸°íƒ€'
    df.loc[df['ì—…ê³„_í‚¤ì›Œë“œ_ê°œìˆ˜'] > 0, 'ì¹´í…Œê³ ë¦¬'] = 'ì—…ê³„'
    df.loc[(df['ì—…ê³„_í‚¤ì›Œë“œ_ê°œìˆ˜'] == 0) & (df['ê²½ì˜_í‚¤ì›Œë“œ_ê°œìˆ˜'] > 0), 'ì¹´í…Œê³ ë¦¬'] = 'ê²½ì˜'

    # ìµœì¢…ì„ íƒì—¬ë¶€ ì»¬ëŸ¼ ì¶”ê°€ (ì‚¬ìš©ì ì…ë ¥ì„ ìœ„í•´ ë¹ˆ ê°’ìœ¼ë¡œ ì´ˆê¸°í™”)
    df['ìµœì¢…ì„ íƒì—¬ë¶€'] = ''

    sorted_df = df.sort_values(by='ì˜ˆì¸¡ì ìˆ˜', ascending=False)
    
    # 'ë³¸ë¬¸' ì»¬ëŸ¼ ì œì™¸í•˜ê³  ì €ì¥
    output_columns = ['ì‹ ë¬¸ì‚¬', 'ì œëª©', 'ë³¸ë¬¸_ìš”ì•½', 'ë§í¬', 'ì¹´í…Œê³ ë¦¬', 'ì˜ˆì¸¡ì ìˆ˜', 'ìµœì¢…ì„ íƒì—¬ë¶€']
    final_df = sorted_df[output_columns]
    
    final_df.to_csv(SCRAPPED_NEWS_TODAY_CSV, index=False, encoding='utf-8-sig')
    print(f"  - ìµœì¢… ê²°ê³¼ê°€ '{SCRAPPED_NEWS_TODAY_CSV}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("--- 2ë‹¨ê³„: ìŠ¤í¬ë© ê°€ì¹˜ ì˜ˆì¸¡ ì™„ë£Œ ---")
    print()
    return True

# --- 3ë‹¨ê³„: ì •ì  íŒŒì¼ ìƒì„± ---
def generate_static_files():
    print("--- 3ë‹¨ê³„: GitHub Pagesìš© ì •ì  íŒŒì¼ ìƒì„± ì‹œì‘ ---")
    
    if not os.path.exists(SCRAPPED_NEWS_TODAY_CSV) or os.path.getsize(SCRAPPED_NEWS_TODAY_CSV) == 0:
        print(f"  ì˜¤ë¥˜: '{SCRAPPED_NEWS_TODAY_CSV}' íŒŒì¼ì´ ë¹„ì–´ìˆê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return False
        
    try:
        print("  [1/3] JSON íŒŒì¼ ìƒì„± ì¤‘...")
        df = pd.read_csv(SCRAPPED_NEWS_TODAY_CSV, encoding='utf-8-sig')
        df.to_json(JSON_OUTPUT_FILE, orient='records', force_ascii=False, indent=4)
        print(f"    '{JSON_OUTPUT_FILE}' ìƒì„± ì™„ë£Œ.")
    except Exception as e:
        print(f"  ì˜¤ë¥˜: JSON ë³€í™˜ ì‹¤íŒ¨ - {e}")
        return False

    try:
        print("  [2/3] HTML íŒŒì¼ ìƒì„± ì¤‘...")
        with open(HTML_TEMPLATE_FILE, 'r', encoding='utf-8') as f: html_content = f.read()
        modified_html = html_content.replace('<script src="/static/js/main.js"></script>', '<script src="static/js/github_pages_main.js"></script>')
        with open(HTML_OUTPUT_FILE, 'w', encoding='utf-8') as f: f.write(modified_html)
        print(f"    '{HTML_OUTPUT_FILE}' ìƒì„± ì™„ë£Œ.")
    except FileNotFoundError:
        print(f"  ì˜¤ë¥˜: HTML í…œí”Œë¦¿ íŒŒì¼ '{HTML_TEMPLATE_FILE}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return False
    except Exception as e:
        print(f"  ì˜¤ë¥˜: HTML íŒŒì¼ ìƒì„± ì‹¤íŒ¨ - {e}")
        return False

    try:
        print("  [3/3] JavaScript íŒŒì¼ ìƒì„± ì¤‘...")
        with open(JS_TEMPLATE_FILE, 'r', encoding='utf-8') as f: js_content = f.read()
        modified_js = js_content.replace("fetch('/api/news')", "fetch('news_data.json')")
        modified_js = modified_js.replace("setInterval(fetchNews, 30000);", "/* ìë™ ìƒˆë¡œê³ ì¹¨ ë¹„í™œì„±í™” */")
        with open(JS_OUTPUT_FILE, 'w', encoding='utf-8') as f: f.write(modified_js)
        print(f"    '{JS_OUTPUT_FILE}' ìƒì„± ì™„ë£Œ.")
    except FileNotFoundError:
        print(f"  ì˜¤ë¥˜: JS í…œí”Œë¦¿ íŒŒì¼ '{JS_TEMPLATE_FILE}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return False
    except Exception as e:
        print(f"  ì˜¤ë¥˜: JavaScript íŒŒì¼ ìƒì„± ì‹¤íŒ¨ - {e}")
        return False

    print("--- 3ë‹¨ê³„: ì •ì  íŒŒì¼ ìƒì„± ì™„ë£Œ ---")
    print()
    return True

# --- ë©”ì¸ ì‹¤í–‰ ë¸”ë¡ ---
if __name__ == "__main__":
    # 1ë‹¨ê³„ ì‹¤í–‰
    processed_articles_df = run_step_one_crawling_and_preprocessing()
    
    # 2ë‹¨ê³„ ì‹¤í–‰
    if not processed_articles_df.empty:
        prediction_success = run_step_two_prediction(processed_articles_df)
        
        # 3ë‹¨ê³„ ì‹¤í–‰
        if prediction_success:
            if generate_static_files():
                print("ğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                print("!!!!! 3ë‹¨ê³„(ì •ì  íŒŒì¼ ìƒì„±)ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. !!!!!")
        else:
            print("!!!!! 2ë‹¨ê³„(ìŠ¤í¬ë© ì˜ˆì¸¡)ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. !!!!!")
    else:
        print("!!!!! 1ë‹¨ê³„(í¬ë¡¤ë§ ë° ì „ì²˜ë¦¬)ì—ì„œ ì²˜ë¦¬í•  ê¸°ì‚¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. !!!!!")
