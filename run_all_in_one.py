1 # -*- coding: utf-8 -*-
     2 # í†µí•© ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ v8 (ìµœì¢…): run_all_in_one.py
     3 # ê¸°ëŠ¥: ìµœìƒìœ„ ê²½ë¡œ êµ¬ì¡°ì— ë§ì¶° ë™ì  í¬ë¡¤ë§, ì¤‘ë³µ ì œê±°, ìŠ¤í¬ë© ê°€ì¹˜ ì˜ˆì¸¡, ì •ì  íŒŒì¼ ìƒì„±ì„ ëª¨ë‘ ìˆ˜í–‰í•©ë‹ˆë‹¤.
     4 
     5 import pandas as pd
     6 import numpy as np
     7 import re
     8 import pickle
     9 import os
    10 import requests
    11 from bs4 import BeautifulSoup
    12 from konlpy.tag import Okt
    13 from sklearn.feature_extraction.text import TfidfVectorizer
    14 from scipy.sparse import hstack
    15 import time
    16 import json
    17 import sys
    18 import io
    19 from tqdm import tqdm
    20 from sklearn.metrics.pairwise import cosine_similarity
    21 
    22 # --- ì „ì—­ ì„¤ì • ë° íŒŒì¼ ê²½ë¡œ (ìµœìƒìœ„ ê²½ë¡œ ê¸°ì¤€) ---
    23 
    24 # SSL ì˜¤ë¥˜ ë°©ì§€ ì„¤ì • (í•„ìš” ì‹œ)
    25 os.environ['CURL_CA_BUNDLE'] = ''
    26 os.environ['REQUESTS_CA_BUNDLE'] = ''
    27 os.environ['HF_HUB_DISABLE_CERTIFICATE_VERIFICATION'] = '1'
    28 
    29 # íŒŒì¼ ê²½ë¡œ ì •ì˜
    30 BASE_DIR = os.path.dirname(os.path.abspath(__file__))
    31 SCRAPPED_NEWS_TODAY_CSV = os.path.join(BASE_DIR, 'scrapped_news_today.csv')
    32 MODEL_PATH = os.path.join(BASE_DIR, 'scrap_model.pkl')
    33 VECTORIZER_PATH = os.path.join(BASE_DIR, 'tfidf_vectorizer.pkl')
    34 JSON_OUTPUT_FILE = os.path.join(BASE_DIR, 'news_data.json')
    35 HTML_TEMPLATE_FILE = os.path.join(BASE_DIR, 'index.html')
    36 HTML_OUTPUT_FILE = os.path.join(BASE_DIR, 'index.html')
    37 JS_TEMPLATE_FILE = os.path.join(BASE_DIR, 'static', 'js', 'main.js')
    38 JS_OUTPUT_FILE = os.path.join(BASE_DIR, 'static', 'js', 'github_pages_main.js')
    39 
    40 # --- ê³µí†µ ì „ì²˜ë¦¬ í•¨ìˆ˜ ë° ê°ì²´ ---
    41 okt = Okt()
    42 def preprocess(text):
    43     """ í…ìŠ¤íŠ¸ì—ì„œ ëª…ì‚¬ë§Œ ì¶”ì¶œí•˜ì—¬ ê³µë°±ìœ¼ë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤. """
    44     return ' '.join(okt.nouns(re.sub(r'[^\ã„±-ã…ã…-ã…£ê°€-í£ ]','', str(text))))
    45 
    46 # --- 1ë‹¨ê³„: ë™ì  í¬ë¡¤ë§, ì¤‘ë³µ ì œê±° ë° ì „ì²˜ë¦¬ ---
    47 
    48 def get_today_articles():
    49     """ 5ëŒ€ ì¼ê°„ì§€ì—ì„œ ì˜¤ëŠ˜ì˜ ì£¼ìš” ê¸°ì‚¬ ëª©ë¡(ì œëª©, ë§í¬, ì‹ ë¬¸ì‚¬)ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤. """
    50     print("--- 1.1: ì£¼ìš” ì¼ê°„ì§€ ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘ ì‹œì‘ ---")
    51     newspapers = {
    52         'í•œêµ­ê²½ì œ': '015', 'ë§¤ì¼ê²½ì œ': '009', 'ë™ì•„ì¼ë³´': '020',
    53         'ì¡°ì„ ì¼ë³´': '023', 'ì¤‘ì•™ì¼ë³´': '025',
    54     }
    55     headers = {'User-Agent': 'Mozilla/5.0'}
    56     all_articles = []
    57     for name, oid in newspapers.items():
    58         try:
    59             url = f"https://media.naver.com/press/{oid}/newspaper"
    60             response = requests.get(url, headers=headers, verify=False, timeout=10)
    61             response.raise_for_status()
    62             soup = BeautifulSoup(response.text, 'html.parser')
    63             links = soup.select('div.sc_offc_lst._paper_article_list a')
    64             for link in links:
    65                 title = link.get_text(strip=True)
    66                 href = link.get('href', '')
    67                 if title and href:
    68                     all_articles.append({
    69                         'ì‹ ë¬¸ì‚¬': name, 'ì œëª©': title,
    70                         'ë§í¬': 'https://media.naver.com' + href if href.startswith('/') else href
    71                     })
    72             time.sleep(0.5)
    73         except Exception as e:
    74             print(f"  âœ— {name} ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
    75     df = pd.DataFrame(all_articles).drop_duplicates(subset=['ë§í¬']).reset_index(drop=True)
    76     print(f"  - ì´ {len(df)}ê°œì˜ ê³ ìœ  ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘ ì™„ë£Œ.")
    77     return df
    78 
    79 def get_article_content(url):
    80     """ ë„¤ì´ë²„ ë‰´ìŠ¤ ë§í¬ì—ì„œ ë³¸ë¬¸ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤. """
    81     try:
    82         headers = {'User-Agent': 'Mozilla/5.0'}
    83         response = requests.get(url, headers=headers, timeout=10, verify=False)
    84         response.raise_for_status()
    85         soup = BeautifulSoup(response.text, 'html.parser')
    86         content_area = soup.select_one('#dic_area, #articeBody, #article_content')
    87         if content_area:
    88             for el in content_area.select('script, style, .reporter_area, .ad_area, .promotion_area, div.byline, a, span.end_photo_org'):
    89                 el.decompose()
    90             return content_area.get_text(strip=True)
    91     except Exception as e:
    92         print(f"    - ë§í¬ í¬ë¡¤ë§ ì‹¤íŒ¨: {url}, ì˜¤ë¥˜: {e}")
    93     return ""
    94 
    95 def run_step_one_crawling_and_preprocessing():
    96     """ í¬ë¡¤ë§, ìš”ì•½, ì¤‘ë³µ ì œê±°, ì „ì²˜ë¦¬ë¥¼ í†µí•© ì‹¤í–‰í•©ë‹ˆë‹¤. """
    97     articles_df = get_today_articles()
    98     if articles_df.empty: return pd.DataFrame()
    99 
   100     print("--- 1.2: ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§ ë° ìš”ì•½ ìƒì„± ---")
   101     crawled_data = []
   102     for _, row in tqdm(articles_df.iterrows(), total=len(articles_df), desc="  - ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§"):
   103         content = get_article_content(row['ë§í¬'])
   104         if content:
   105             crawled_data.append([
   106                 row['ì‹ ë¬¸ì‚¬'], row['ì œëª©'], row['ë§í¬'], content, content[:400] + "..."
   107             ])
   108         time.sleep(0.5)
   109     
   110     if not crawled_data:
   111         print("í¬ë¡¤ë§ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
   112         return pd.DataFrame()
   113 
   114     df = pd.DataFrame(crawled_data, columns=['ì‹ ë¬¸ì‚¬', 'ì œëª©', 'ë§í¬', 'ë³¸ë¬¸', 'ë³¸ë¬¸_ìš”ì•½'])
   115 
   116     print("--- 1.3: ë‚´ìš© ê¸°ë°˜ ì¤‘ë³µ ê¸°ì‚¬ ì œê±° (ìœ ì‚¬ë„ 0.6 ê¸°ì¤€) ---")
   117     if not df.empty and 'ë³¸ë¬¸' in df.columns and not df['ë³¸ë¬¸'].isnull().all():
   118         df['processed_text_for_dedup'] = df['ë³¸ë¬¸'].apply(preprocess)
   119         vectorizer = TfidfVectorizer(min_df=1, ngram_range=(1, 2))
   120         tfidf_matrix = vectorizer.fit_transform(df['processed_text_for_dedup'])
   121         cosine_sim = cosine_similarity(tfidf_matrix)
   122         
   123         df['ë³¸ë¬¸_ê¸¸ì´_dedup'] = df['ë³¸ë¬¸'].str.len()
   124         df = df.sort_values(by='ë³¸ë¬¸_ê¸¸ì´_dedup', ascending=False).reset_index(drop=True)
   125         
   126         tfidf_matrix_sorted = vectorizer.fit_transform(df['processed_text_for_dedup'])
   127         cosine_sim_sorted = cosine_similarity(tfidf_matrix_sorted)
   128         
   129         to_drop = set()
   130         for i in range(len(cosine_sim_sorted)):
   131             if i in to_drop: continue
   132             for j in range(i + 1, len(cosine_sim_sorted)):
   133                 if j in to_drop: continue
   134                 if cosine_sim_sorted[i, j] > 0.6:
   135                     to_drop.add(j)
   136         
   137         if to_drop:
   138             print(f"   - {len(to_drop)}ê°œì˜ ìœ ì‚¬ ê¸°ì‚¬ë¥¼ ì œê±°í–ˆìŠµë‹ˆë‹¤.")
   139             df.drop(index=list(to_drop), inplace=True)
   140         
   141         df.drop(columns=['processed_text_for_dedup', 'ë³¸ë¬¸_ê¸¸ì´_dedup'], inplace=True)
   142 
   143     print(f"   - ìµœì¢… ë¶„ì„ ëŒ€ìƒ ê¸°ì‚¬: {len(df)}ê°œ")
   144     print("--- 1ë‹¨ê³„: í¬ë¡¤ë§ ë° ì „ì²˜ë¦¬ ì™„ë£Œ ---")
   145     print()
   146     return df
   147 
   148 # --- 2ë‹¨ê³„: ìŠ¤í¬ë© ê°€ì¹˜ ì˜ˆì¸¡ ---
   149 
   150 def run_step_two_prediction(df):
   151     """ í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìŠ¤í¬ë© ê°€ì¹˜ë¥¼ ì˜ˆì¸¡í•˜ê³  ìµœì¢… ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥í•©ë‹ˆë‹¤. """
   152     if df.empty:
   153         print("2ë‹¨ê³„ ì‹¤íŒ¨: ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
   154         return False
   155         
   156     print("--- 2.1: ëª¨ë¸ ë° Vectorizer ë¡œë“œ ---")
   157     try:
   158         with open(MODEL_PATH, "rb") as f: model = pickle.load(f)
   159         with open(VECTORIZER_PATH, "rb") as f: tfidf_vectorizer = pickle.load(f)
   160     except FileNotFoundError:
   161         print(f"ì˜¤ë¥˜: ëª¨ë¸ íŒŒì¼({MODEL_PATH}) ë˜ëŠ” Vectorizer íŒŒì¼({VECTORIZER_PATH})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
   162         return False
   163 
   164     print("--- 2.2: í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° íŠ¹ì„± ì¶”ì¶œ ---")
   165     keywords = {
   166         'ì—…ê³„': ['ì‹í’ˆ', 'í™”í•™', 'ë°”ì´ì˜¤', 'íŒ¨í‚¤ì§•', 'í”Œë¼ìŠ¤í‹±', 'í•­ì•”ì œ', 'ë°°í„°ë¦¬', 'ì¹œí™˜ê²½', 'Dë¨', 'ì‚¼ì–‘', 'ì´ˆìˆœìˆ˜', 'ì œì•½ì‚¬', 'ìˆ™ì·¨', 'ìƒì¾Œí™˜', 'ì„¤íƒ•', 'ì¹¼ë¡œë¦¬', 'ì‚¼ì–‘ê·¸ë£¹', 'ì‚¼ì–‘ì‚¬', 'ì‚¼ì–‘íŒ¨í‚¤ì§•', 'ì‚¼ì–‘ì—”ì”¨ì¼', 'ì‚¼ì–‘ë°”ì´ì˜¤íŒœ'],
   167         'ê²½ì˜': ['ê²½ì˜', 'ê²½ì œ', 'í™˜ìœ¨', 'M&A', 'ì¸ìˆ˜', 'íˆ¬ì', 'ì‹¤ì ', 'í•œì¼ê²½ì œí˜‘íšŒ', 'ì¡°ì§ë¬¸í™”', 'ë¬´ì—­']
   168     }
   169 
   170     df['processed_title'] = df['ì œëª©'].apply(preprocess)
   171     df['processed_text'] = df.apply(lambda r: preprocess(r['ì œëª©'] + " " + r['ë³¸ë¬¸']), axis=1)
   172     
   173     df['ì—…ê³„_í‚¤ì›Œë“œ_ê°œìˆ˜'] = df['processed_text'].apply(lambda x: sum(k in x for k in keywords['ì—…ê³„']))
   174     df['ê²½ì˜_í‚¤ì›Œë“œ_ê°œìˆ˜'] = df['processed_text'].apply(lambda x: sum(k in x for k in keywords['ê²½ì˜']))
   175     df['ì—…ê³„_í‚¤ì›Œë“œ_ì œëª©_ê°œìˆ˜'] = df['processed_title'].apply(lambda x: sum(k in x for k in keywords['ì—…ê³„']))
   176     df['ê²½ì˜_í‚¤ì›Œë“œ_ì œëª©_ê°œìˆ˜'] = df['processed_title'].apply(lambda x: sum(k in x for k in keywords['ê²½ì˜']))
   177     df['ë³¸ë¬¸_ê¸¸ì´'] = df['ë³¸ë¬¸'].str.len().replace(0, 1)
   178     df['ì—…ê³„_í‚¤ì›Œë“œ_ë°€ë„'] = df['ì—…ê³„_í‚¤ì›Œë“œ_ê°œìˆ˜'] / df['ë³¸ë¬¸_ê¸¸ì´']
   179     df['ê²½ì˜_í‚¤ì›Œë“œ_ë°€ë„'] = df['ê²½ì˜_í‚¤ì›Œë“œ_ê°œìˆ˜'] / df['ë³¸ë¬¸_ê¸¸ì´']
   180 
   181     text_features = tfidf_vectorizer.transform(df['processed_text'])
   182     metadata_features = df[['ì—…ê³„_í‚¤ì›Œë“œ_ê°œìˆ˜', 'ê²½ì˜_í‚¤ì›Œë“œ_ê°œìˆ˜', 'ì—…ê³„_í‚¤ì›Œë“œ_ì œëª©_ê°œìˆ˜', 'ê²½ì˜_í‚¤ì›Œë“œ_ì œëª©_ê°œìˆ˜', 'ë³¸ë¬¸_ê¸¸ì´', 'ì—…ê³„_í‚¤ì›Œë“œ_ë°€ë„', 'ê²½ì˜_í‚¤ì›Œë“œ_ë°€ë„']].values
   183     
   184     X_new = hstack([text_features, metadata_features * 0.5]).tocsr()
   185 
   186     print("--- 2.3: ìŠ¤í¬ë© ê°€ì¹˜ ì ìˆ˜ ì˜ˆì¸¡ ë° ê²°ê³¼ ì €ì¥ ---")
   187     df['ì˜ˆì¸¡ì ìˆ˜'] = model.predict_proba(X_new)[:, 1]
   188     
   189     df['ì¹´í…Œê³ ë¦¬'] = 'ê¸°íƒ€'
   190     df.loc[df['ì—…ê³„_í‚¤ì›Œë“œ_ê°œìˆ˜'] > 0, 'ì¹´í…Œê³ ë¦¬'] = 'ì—…ê³„'
   191     df.loc[(df['ì—…ê³„_í‚¤ì›Œë“œ_ê°œìˆ˜'] == 0) & (df['ê²½ì˜_í‚¤ì›Œë“œ_ê°œìˆ˜'] > 0), 'ì¹´í…Œê³ ë¦¬'] = 'ê²½ì˜'
   192 
   193     # ìµœì¢…ì„ íƒì—¬ë¶€ ì»¬ëŸ¼ ì¶”ê°€ (ì‚¬ìš©ì ì…ë ¥ì„ ìœ„í•´ ë¹ˆ ê°’ìœ¼ë¡œ ì´ˆê¸°í™”)
   194     df['ìµœì¢…ì„ íƒì—¬ë¶€'] = ''
   195 
   196     sorted_df = df.sort_values(by='ì˜ˆì¸¡ì ìˆ˜', ascending=False)
   197     
   198     # 'ë³¸ë¬¸' ì»¬ëŸ¼ ì œì™¸í•˜ê³  ì €ì¥
   199     output_columns = ['ì‹ ë¬¸ì‚¬', 'ì œëª©', 'ë³¸ë¬¸_ìš”ì•½', 'ë§í¬', 'ì¹´í…Œê³ ë¦¬', 'ì˜ˆì¸¡ì ìˆ˜', 'ìµœì¢…ì„ íƒì—¬ë¶€']
   200     final_df = sorted_df[output_columns]
   201     
   202     final_df.to_csv(SCRAPPED_NEWS_TODAY_CSV, index=False, encoding='utf-8-sig')
   203     print(f"  - ìµœì¢… ê²°ê³¼ê°€ '{SCRAPPED_NEWS_TODAY_CSV}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
   204     print("--- 2ë‹¨ê³„: ìŠ¤í¬ë© ê°€ì¹˜ ì˜ˆì¸¡ ì™„ë£Œ ---")
   205     print()
   206     return True
   207 
   208 # --- 3ë‹¨ê³„: ì •ì  íŒŒì¼ ìƒì„± ---
   209 def generate_static_files():
   210     print("--- 3ë‹¨ê³„: GitHub Pagesìš© ì •ì  íŒŒì¼ ìƒì„± ì‹œì‘ ---")
   211     
   212     if not os.path.exists(SCRAPPED_NEWS_TODAY_CSV) or os.path.getsize(SCRAPPED_NEWS_TODAY_CSV) == 0:
   213         print(f"  ì˜¤ë¥˜: '{SCRAPPED_NEWS_TODAY_CSV}' íŒŒì¼ì´ ë¹„ì–´ìˆê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
   214         return False
   215         
   216     try:
   217         print("  [1/3] JSON íŒŒì¼ ìƒì„± ì¤‘...")
   218         df = pd.read_csv(SCRAPPED_NEWS_TODAY_CSV, encoding='utf-8-sig')
   219         df.to_json(JSON_OUTPUT_FILE, orient='records', force_ascii=False, indent=4)
   220         print(f"    '{JSON_OUTPUT_FILE}' ìƒì„± ì™„ë£Œ.")
   221     except Exception as e:
   222         print(f"  ì˜¤ë¥˜: JSON ë³€í™˜ ì‹¤íŒ¨ - {e}")
   223         return False
   224 
   225     # HTML íŒŒì¼ì€ ì´ë¯¸ ì˜¬ë°”ë¥¸ JSë¥¼ ì°¸ì¡°í•˜ë¯€ë¡œ ë³„ë„ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ.
   226     print("  [2/3] HTML íŒŒì¼ ì²˜ë¦¬ ê±´ë„ˆëœ€ (index.htmlì´ ì´ë¯¸ ì˜¬ë°”ë¥¸ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì°¸ì¡°).")
   227 
   228     try:
   229         print("  [3/3] JavaScript íŒŒì¼ ìƒì„± ì¤‘...")
   230         # JS ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
   231         os.makedirs(os.path.dirname(JS_OUTPUT_FILE), exist_ok=True)
   232         with open(JS_TEMPLATE_FILE, 'r', encoding='utf-8') as f: js_content = f.read()
   233         modified_js = js_content.replace("fetch('/api/news')", "fetch('news_data.json')")
   234         modified_js = modified_js.replace("setInterval(fetchNews, 30000);", "/* ìë™ ìƒˆë¡œê³ ì¹¨ ë¹„í™œì„±í™” */")
   235         with open(JS_OUTPUT_FILE, 'w', encoding='utf-8') as f: f.write(modified_js)
   236         print(f"    '{JS_OUTPUT_FILE}' ìƒì„± ì™„ë£Œ.")
   237     except FileNotFoundError:
   238         print(f"  ì˜¤ë¥˜: JS í…œí”Œë¦¿ íŒŒì¼ '{JS_TEMPLATE_FILE}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
   239         return False
   240     except Exception as e:
   241         print(f"  ì˜¤ë¥˜: JavaScript íŒŒì¼ ìƒì„± ì‹¤íŒ¨ - {e}")
   242         return False
   243 
   244     print("--- 3ë‹¨ê³„: ì •ì  íŒŒì¼ ìƒì„± ì™„ë£Œ ---")
   245     print()
   246     return True
   247 
   248 # --- ë©”ì¸ ì‹¤í–‰ ë¸”ë¡ ---
   249 if __name__ == "__main__":
   250     # 1ë‹¨ê³„ ì‹¤í–‰
   251     processed_articles_df = run_step_one_crawling_and_preprocessing()
   252     
   253     # 2ë‹¨ê³„ ì‹¤í–‰
   254     if not processed_articles_df.empty:
   255         prediction_success = run_step_two_prediction(processed_articles_df)
   256         
   257         # 3ë‹¨ê³„ ì‹¤í–‰
   258         if prediction_success:
   259             if generate_static_files():
   260                 print("ğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
   261             else:
   262                 print("!!!!! 3ë‹¨ê³„(ì •ì  íŒŒì¼ ìƒì„±)ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. !!!!!")
   263         else:
   264             print("!!!!! 2ë‹¨ê³„(ìŠ¤í¬ë© ì˜ˆì¸¡)ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. !!!!!")
   265     else:
   266         print("!!!!! 1ë‹¨ê³„(í¬ë¡¤ë§ ë° ì „ì²˜ë¦¬)ì—ì„œ ì²˜ë¦¬í•  ê¸°ì‚¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. !!!!!")
