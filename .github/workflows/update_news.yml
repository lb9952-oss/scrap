     1 name: Daily News Update
     2 
     3 on:
     4   schedule:
     5     - cron: '0 21 * * *'
     6   workflow_dispatch:
     7 
     8 jobs:
     9   build-and-commit:
    10     runs-on: ubuntu-latest
    11     permissions:
    12       contents: write
    13 
    14     steps:
    15       - name: Checkout repository
    16         uses: actions/checkout@v4
    17 
    18       - name: Set up JDK 17
    19         uses: actions/setup-java@v4
    20         with:
    21           java-version: '17'
    22           distribution: 'temurin'
    23 
    24       - name: Set up Python
    25         uses: actions/setup-python@v5       
    26         with:
    27           python-version: '3.9'
    28 
    29       - name: Install Python dependencies   
    30         run: pip install -r requirements.txt
    31 
    32       - name: Run Python Script
    33         run: |
    34           python -c """
    35           import pandas as pd
    36           import numpy as np
    37           import re
    38           import pickle
    39           import os
    40           import requests
    41           from bs4 import BeautifulSoup
    42           from konlpy.tag import Okt
    43           from sklearn.feature_extraction.text import TfidfVectorizer
    44           from scipy.sparse import hstack
    45           import time
    46           import json
    47           from tqdm import tqdm
    48           from sklearn.metrics.pairwise import cosine_similarity
    49 
    50           os.environ['CURL_CA_BUNDLE'] = ''
    51           os.environ['REQUESTS_CA_BUNDLE'] = ''
    52           os.environ['HF_HUB_DISABLE_CERTIFICATE_VERIFICATION'] = '1'
    53 
    54           BASE_DIR = os.getcwd()
    55           SCRAPPED_NEWS_TODAY_CSV = os.path.join(BASE_DIR, 'scrapped_news_today.csv')
    56           MODEL_PATH = os.path.join(BASE_DIR, 'scrap_model.pkl')
    57           VECTORIZER_PATH = os.path.join(BASE_DIR, 'tfidf_vectorizer.pkl')
    58           JSON_OUTPUT_FILE = os.path.join(BASE_DIR, 'news_data.json')
    59           HTML_TEMPLATE_FILE = os.path.join(BASE_DIR, 'index.html')
    60           HTML_OUTPUT_FILE = os.path.join(BASE_DIR, 'index.html')
    61           JS_TEMPLATE_FILE = os.path.join(BASE_DIR, 'static', 'js', 'main.js')
    62           JS_OUTPUT_FILE = os.path.join(BASE_DIR, 'static', 'js', 'github_pages_main.js')
    63 
    64           okt = Okt()
    65           def preprocess(text):
    66               return ' '.join(okt.nouns(re.sub(r'[^\ã„±-ã…ã…-ã…£ê°€-í£ ]','', str(text))))
    67 
    68           def get_today_articles():
    69               print('--- 1.1: ì£¼ìš” ì¼ê°„ì§€ ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘ ì‹œì‘ ---')
    70               newspapers = {'í•œêµ­ê²½ì œ': '015', 'ë§¤ì¼ê²½ì œ': '009', 'ë™ì•„ì¼ë³´': '020', 'ì¡°ì„ ì¼ë³´': '023', 'ì¤‘ì•™ì¼ë³´': '025'}
    71               headers = {'User-Agent': 'Mozilla/5.0'}
    72               all_articles = []
    73               for name, oid in newspapers.items():
    74                   try:
    75                       url = f"https://media.naver.com/press/{oid}/newspaper"
    76                       response = requests.get(url, headers=headers, verify=False, timeout=10)
    77                       response.raise_for_status()
    78                       soup = BeautifulSoup(response.text, 'html.parser')
    79                       links = soup.select('div.sc_offc_lst._paper_article_list a')
    80                       for link in links:
    81                           title = link.get_text(strip=True)
    82                           href = link.get('href', '')
    83                           if title and href:
    84                               all_articles.append({'ì‹ ë¬¸ì‚¬': name, 'ì œëª©': title, 'ë§í¬': 'https://media.naver.com' + href if href.startswith('/') else href})
    85                       time.sleep(0.5)
    86                   except Exception as e:
    87                       print(f'  - {name} ìˆ˜ì§‘ ì˜¤ë¥˜: {e}')
    88               df = pd.DataFrame(all_articles).drop_duplicates(subset=['ë§í¬']).reset_index(drop=True)
    89               print(f'  - ì´ {len(df)}ê°œì˜ ê³ ìœ  ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘ ì™„ë£Œ.')
    90               return df
    91 
    92           def get_article_content(url):
    93               try:
    94                   headers = {'User-Agent': 'Mozilla/5.0'}
    95                   response = requests.get(url, headers=headers, timeout=10, verify=False)
    96                   response.raise_for_status()
    97                   soup = BeautifulSoup(response.text, 'html.parser')
    98                   content_area = soup.select_one('#dic_area, #articeBody, #article_content')
    99                   if content_area:
   100                       for el in content_area.select('script, style, .reporter_area, .ad_area, .promotion_area, div.byline, a, span.end_photo_org'):
   101                           el.decompose()
   102                       return content_area.get_text(strip=True)
   103               except Exception as e:
   104                   print(f'    - ë§í¬ í¬ë¡¤ë§ ì‹¤íŒ¨: {url}, ì˜¤ë¥˜: {e}')
   105               return ''
   106 
   107           def run_step_one_crawling_and_preprocessing():
   108               articles_df = get_today_articles()
   109               if articles_df.empty: return pd.DataFrame()
   110               print('--- 1.2: ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§ ë° ìš”ì•½ ìƒì„± ---')
   111               crawled_data = []
   112               for _, row in tqdm(articles_df.iterrows(), total=len(articles_df), desc='  - ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§'):
   113                   content = get_article_content(row['ë§í¬'])
   114                   if content:
   115                       crawled_data.append([row['ì‹ ë¬¸ì‚¬'], row['ì œëª©'], row['ë§í¬'], content, content[:400] + '...'])
   116                   time.sleep(0.5)
   117               if not crawled_data:
   118                   print('í¬ë¡¤ë§ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.')
   119                   return pd.DataFrame()
   120               df = pd.DataFrame(crawled_data, columns=['ì‹ ë¬¸ì‚¬', 'ì œëª©', 'ë§í¬', 'ë³¸ë¬¸', 'ë³¸ë¬¸_ìš”ì•½'])
   121               print('--- 1.3: ë‚´ìš© ê¸°ë°˜ ì¤‘ë³µ ê¸°ì‚¬ ì œê±° (ìœ ì‚¬ë„ 0.6 ê¸°ì¤€) ---')
   122               if not df.empty and 'ë³¸ë¬¸' in df.columns and not df['ë³¸ë¬¸'].isnull().all():
   123                   df['processed_text_for_dedup'] = df['ë³¸ë¬¸'].apply(preprocess)
   124                   vectorizer = TfidfVectorizer(min_df=1, ngram_range=(1, 2))
   125                   tfidf_matrix = vectorizer.fit_transform(df['processed_text_for_dedup'])
   126                   cosine_sim = cosine_similarity(tfidf_matrix)
   127                   df['ë³¸ë¬¸_ê¸¸ì´_dedup'] = df['ë³¸ë¬¸'].str.len()
   128                   df = df.sort_values(by='ë³¸ë¬¸_ê¸¸ì´_dedup', ascending=False).reset_index(drop=True)
   129                   tfidf_matrix_sorted = vectorizer.fit_transform(df['processed_text_for_dedup'])
   130                   cosine_sim_sorted = cosine_similarity(tfidf_matrix_sorted)
   131                   to_drop = set()
   132                   for i in range(len(cosine_sim_sorted)):
   133                       if i in to_drop: continue
   134                       for j in range(i + 1, len(cosine_sim_sorted)):
   135                           if j in to_drop: continue
   136                           if cosine_sim_sorted[i, j] > 0.6:
   137                               to_drop.add(j)
   138                   if to_drop:
   139                       print(f'   - {len(to_drop)}ê°œì˜ ìœ ì‚¬ ê¸°ì‚¬ë¥¼ ì œê±°í–ˆìŠµë‹ˆë‹¤.')
   140                       df.drop(index=list(to_drop), inplace=True)
   141                   df.drop(columns=['processed_text_for_dedup', 'ë³¸ë¬¸_ê¸¸ì´_dedup'], inplace=True)
   142               print(f'   - ìµœì¢… ë¶„ì„ ëŒ€ìƒ ê¸°ì‚¬: {len(df)}ê°œ')
   143               print('--- 1ë‹¨ê³„: í¬ë¡¤ë§ ë° ì „ì²˜ë¦¬ ì™„ë£Œ ---')
   144               print()
   145               return df
   146 
   147           def run_step_two_prediction(df):
   148               if df.empty:
   149                   print('2ë‹¨ê³„ ì‹¤íŒ¨: ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.')
   150                   return False
   151               print('--- 2.1: ëª¨ë¸ ë° Vectorizer ë¡œë“œ ---')
   152               try:
   153                   with open(MODEL_PATH, 'rb') as f: model = pickle.load(f)
   154                   with open(VECTORIZER_PATH, 'rb') as f: tfidf_vectorizer = pickle.load(f)
   155               except FileNotFoundError:
   156                   print(f'ì˜¤ë¥˜: ëª¨ë¸ íŒŒì¼({MODEL_PATH}) ë˜ëŠ” Vectorizer íŒŒì¼({VECTORIZER_PATH})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
   157                   return False
   158               print('--- 2.2: í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° íŠ¹ì„± ì¶”ì¶œ ---')
   159               keywords = {'ì—…ê³„': ['ì‹í’ˆ', 'í™”í•™', 'ë°”ì´ì˜¤', 'íŒ¨í‚¤ì§•', 'í”Œë¼ìŠ¤í‹±', 'í•­ì•”ì œ', 'ë°°í„°ë¦¬', 'ì¹œí™˜ê²½', 'Dë¨', 'ì‚¼ì–‘', 'ì´ˆìˆœìˆ˜', 'ì œì•½ì‚¬', 'ìˆ™ì·¨', 'ìƒì¾Œí™˜', 'ì„¤íƒ•', 'ì¹¼ë¡œë¦¬', 'ì‚¼ì–‘ê·¸ë£¹', 'ì‚¼ì–‘ì‚¬', 'ì‚¼ì–‘íŒ¨í‚¤ì§•', 'ì‚¼ì–‘ì—”ì”¨ì¼', 'ì‚¼ì–‘ë°”ì´ì˜¤íŒœ'], 'ê²½ì˜': ['ê²½ì˜', 'ê²½ì œ',
       'í™˜ìœ¨', 'M&A', 'ì¸ìˆ˜', 'íˆ¬ì', 'ì‹¤ì ', 'í•œì¼ê²½ì œí˜‘íšŒ', 'ì¡°ì§ë¬¸í™”', 'ë¬´ì—­']}
   160               df['processed_title'] = df['ì œëª©'].apply(preprocess)
   161               df['processed_text'] = df.apply(lambda r: preprocess(r['ì œëª©'] + ' ' + r['ë³¸ë¬¸']), axis=1)
   162               df['ì—…ê³„_í‚¤ì›Œë“œ_ê°œìˆ˜'] = df['processed_text'].apply(lambda x: sum(k in x for k in keywords['ì—…ê³„']))
   163               df['ê²½ì˜_í‚¤ì›Œë“œ_ê°œìˆ˜'] = df['processed_text'].apply(lambda x: sum(k in x for k in keywords['ê²½ì˜']))
   164               df['ì—…ê³„_í‚¤ì›Œë“œ_ì œëª©_ê°œìˆ˜'] = df['processed_title'].apply(lambda x: sum(k in x for k in keywords['ì—…ê³„']))
   165               df['ê²½ì˜_í‚¤ì›Œë“œ_ì œëª©_ê°œìˆ˜'] = df['processed_title'].apply(lambda x: sum(k in x for k in keywords['ê²½ì˜']))
   166               df['ë³¸ë¬¸_ê¸¸ì´'] = df['ë³¸ë¬¸'].str.len().replace(0, 1)
   167               df['ì—…ê³„_í‚¤ì›Œë“œ_ë°€ë„'] = df['ì—…ê³„_í‚¤ì›Œë“œ_ê°œìˆ˜'] / df['ë³¸ë¬¸_ê¸¸ì´']
   168               df['ê²½ì˜_í‚¤ì›Œë“œ_ë°€ë„'] = df['ê²½ì˜_í‚¤ì›Œë“œ_ê°œìˆ˜'] / df['ë³¸ë¬¸_ê¸¸ì´']
   169               text_features = tfidf_vectorizer.transform(df['processed_text'])
   170               metadata_features = df[['ì—…ê³„_í‚¤ì›Œë“œ_ê°œìˆ˜', 'ê²½ì˜_í‚¤ì›Œë“œ_ê°œìˆ˜', 'ì—…ê³„_í‚¤ì›Œë“œ_ì œëª©_ê°œìˆ˜', 'ê²½ì˜_í‚¤ì›Œë“œ_ì œëª©_ê°œìˆ˜', 'ë³¸ë¬¸_ê¸¸ì´', 'ì—…ê³„_í‚¤ì›Œë“œ_ë°€ë„', 'ê²½ì˜_í‚¤ì›Œë“œ_ë°€ë„']].values
   171               X_new = hstack([text_features, metadata_features * 0.5]).tocsr()
   172               print('--- 2.3: ìŠ¤í¬ë© ê°€ì¹˜ ì ìˆ˜ ì˜ˆì¸¡ ë° ê²°ê³¼ ì €ì¥ ---')
   173               df['ì˜ˆì¸¡ì ìˆ˜'] = model.predict_proba(X_new)[:, 1]
   174               df['ì¹´í…Œê³ ë¦¬'] = 'ê¸°íƒ€'
   175               df.loc[df['ì—…ê³„_í‚¤ì›Œë“œ_ê°œìˆ˜'] > 0, 'ì¹´í…Œê³ ë¦¬'] = 'ì—…ê³„'
   176               df.loc[(df['ì—…ê³„_í‚¤ì›Œë“œ_ê°œìˆ˜'] == 0) & (df['ê²½ì˜_í‚¤ì›Œë“œ_ê°œìˆ˜'] > 0), 'ì¹´í…Œê³ ë¦¬'] = 'ê²½ì˜'
   177               df['ìµœì¢…ì„ íƒì—¬ë¶€'] = ''
   178               sorted_df = df.sort_values(by='ì˜ˆì¸¡ì ìˆ˜', ascending=False)
   179               output_columns = ['ì‹ ë¬¸ì‚¬', 'ì œëª©', 'ë³¸ë¬¸_ìš”ì•½', 'ë§í¬', 'ì¹´í…Œê³ ë¦¬', 'ì˜ˆì¸¡ì ìˆ˜', 'ìµœì¢…ì„ íƒì—¬ë¶€']
   180               final_df = sorted_df[output_columns]
   181               final_df.to_csv(SCRAPPED_NEWS_TODAY_CSV, index=False, encoding='utf-8-sig')
   182               print(f'  - ìµœì¢… ê²°ê³¼ê°€ {SCRAPPED_NEWS_TODAY_CSV} íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.')
   183               print('--- 2ë‹¨ê³„: ìŠ¤í¬ë© ê°€ì¹˜ ì˜ˆì¸¡ ì™„ë£Œ ---')
   184               print()
   185               return True
   186 
   187           def generate_static_files():
   188               print('--- 3ë‹¨ê³„: GitHub Pagesìš© ì •ì  íŒŒì¼ ìƒì„± ì‹œì‘ ---')
   189               if not os.path.exists(SCRAPPED_NEWS_TODAY_CSV) or os.path.getsize(SCRAPPED_NEWS_TODAY_CSV) == 0:
   190                   print(f'  ì˜¤ë¥˜: {SCRAPPED_NEWS_TODAY_CSV} íŒŒì¼ì´ ë¹„ì–´ìˆê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.')
   191                   return False
   192               try:
   193                   print('  [1/3] JSON íŒŒì¼ ìƒì„± ì¤‘...')
   194                   df = pd.read_csv(SCRAPPED_NEWS_TODAY_CSV, encoding='utf-8-sig')
   195                   df.to_json(JSON_OUTPUT_FILE, orient='records', force_ascii=False, indent=4)
   196                   print(f'    {JSON_OUTPUT_FILE} ìƒì„± ì™„ë£Œ.')
   197               except Exception as e:
   198                   print(f'  ì˜¤ë¥˜: JSON ë³€í™˜ ì‹¤íŒ¨ - {e}')
   199                   return False
   200               print('  [2/3] HTML íŒŒì¼ ì²˜ë¦¬ ê±´ë„ˆëœ€ (index.htmlì´ ì´ë¯¸ ì˜¬ë°”ë¥¸ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì°¸ì¡°).')
   201               try:
   202                   print('  [3/3] JavaScript íŒŒì¼ ìƒì„± ì¤‘...')
   203                   os.makedirs(os.path.dirname(JS_OUTPUT_FILE), exist_ok=True)
   204                   with open(JS_TEMPLATE_FILE, 'r', encoding='utf-8') as f: js_content = f.read()
   205                   modified_js = js_content.replace("fetch('/api/news')", "fetch('news_data.json')")
   206                   modified_js = modified_js.replace("setInterval(fetchNews, 30000);", '/* ìë™ ìƒˆë¡œê³ ì¹¨ ë¹„í™œì„±í™” */')
   207                   with open(JS_OUTPUT_FILE, 'w', encoding='utf-8') as f: f.write(modified_js)
   208                   print(f'    {JS_OUTPUT_FILE} ìƒì„± ì™„ë£Œ.')
   209               except FileNotFoundError:
   210                   print(f'  ì˜¤ë¥˜: JS í…œí”Œë¦¿ íŒŒì¼ {JS_TEMPLATE_FILE}ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
   211                   return False
   212               except Exception as e:
   213                   print(f'  ì˜¤ë¥˜: JavaScript íŒŒì¼ ìƒì„± ì‹¤íŒ¨ - {e}')
   214                   return False
   215               print('--- 3ë‹¨ê³„: ì •ì  íŒŒì¼ ìƒì„± ì™„ë£Œ ---')
   216               print()
   217               return True
   218 
   219           if __name__ == '__main__':
   220               processed_articles_df = run_step_one_crawling_and_preprocessing()
   221               if not processed_articles_df.empty:
   222                   prediction_success = run_step_two_prediction(processed_articles_df)
   223                   if prediction_success:
   224                       if generate_static_files():
   225                           print('ğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.')
   226                       else:
   227                           print('!!!!! 3ë‹¨ê³„(ì •ì  íŒŒì¼ ìƒì„±)ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. !!!!!')
   228                   else:
   229                       print('!!!!! 2ë‹¨ê³„(ìŠ¤í¬ë© ì˜ˆì¸¡)ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. !!!!!')
   230               else:
   231                   print('!!!!! 1ë‹¨ê³„(í¬ë¡¤ë§ ë° ì „ì²˜ë¦¬)ì—ì„œ ì²˜ë¦¬í•  ê¸°ì‚¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. !!!!!')
   232           """
   233 
   234       - name: Commit and push if it changed
   235         uses: stefanzweifel/git-auto-commit-action@v5
   236         with:
   237           commit_message: 'chore: Daily news data updated'
   238           file_pattern: 'news_data.json index.html static/js/github_pages_main.js scrapped_news_today.csv'
