 1 # .github/workflows/update_news.yml      
     2 
     3 name: Daily News Update
     4 
     5 on:
     6   schedule:
     7     - cron: '0 21 * * *'
     8   workflow_dispatch:
     9 
    10 jobs:
    11   build-and-commit:
    12     runs-on: ubuntu-latest
    13     permissions:
    14       contents: write
    15 
    16     steps:
    17       - name: Checkout repository        
    18         uses: actions/checkout@v4        
    19 
    20       - name: Set up JDK 17
    21         uses: actions/setup-java@v4      
    22         with:
    23           java-version: '17'
    24           distribution: 'temurin'        
    25 
    26       - name: Set up Python
    27         uses: actions/setup-python@v5    
    28         with:
    29           python-version: '3.9'
    30 
    31       - name: Install Python dependencies
    32         run: pip install -r requirements.txt
    33 
    34       - name: Run Python Script
    35         run: |
    36           python -c """
    37           # -*- coding: utf-8 -*-
    38           import pandas as pd
    39           import numpy as np
    40           import re
    41           import pickle
    42           import os
    43           import requests
    44           from bs4 import BeautifulSoup
    45           from konlpy.tag import Okt
    46           from sklearn.feature_extraction.text import TfidfVectorizer
    47           from scipy.sparse import hstack
    48           import time
    49           import json
    50           from tqdm import tqdm
    51           from sklearn.metrics.pairwise import cosine_similarity
    52 
    53           # --- ì „ì—­ ì„¤ì • ë° íŒŒì¼ ê²½ë¡œ (ìµœìƒìœ„ ê²½ë¡œ ê¸°ì¤€) ---
    54           os.environ['CURL_CA_BUNDLE'] = ''
    55           os.environ['REQUESTS_CA_BUNDLE'] = ''
    56           os.environ['HF_HUB_DISABLE_CERTIFICATE_VERIFICATION'] = '1'
    57 
    58           BASE_DIR = os.getcwd() # ì›Œí¬í”Œë¡œìš°ì˜ í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ (ì €ì¥ì†Œ ë£¨íŠ¸)
    59           SCRAPPED_NEWS_TODAY_CSV = os.path.join(BASE_DIR, 'scrapped_news_today.csv')
    60           MODEL_PATH = os.path.join(BASE_DIR, 'scrap_model.pkl')
    61           VECTORIZER_PATH = os.path.join(BASE_DIR, 'tfidf_vectorizer.pkl')
    62           JSON_OUTPUT_FILE = os.path.join(BASE_DIR, 'news_data.json')
    63           HTML_TEMPLATE_FILE = os.path.join(BASE_DIR, 'index.html')
    64           HTML_OUTPUT_FILE = os.path.join(BASE_DIR, 'index.html')
    65           JS_TEMPLATE_FILE = os.path.join(BASE_DIR, 'static', 'js', 'main.js')
    66           JS_OUTPUT_FILE = os.path.join(BASE_DIR, 'static', 'js', 'github_pages_main.js')
    67 
    68           okt = Okt()
    69           def preprocess(text):
    70               return ' '.join(okt.nouns(re.sub(r'[^\ã„±-ã…ã…-ã…£ê°€-í£ ]','', str(text))))
    71 
    72           def get_today_articles():
    73               print('--- 1.1: ì£¼ìš” ì¼ê°„ì§€ ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘ ì‹œì‘ ---')
    74               newspapers = {'í•œêµ­ê²½ì œ': '015', 'ë§¤ì¼ê²½ì œ': '009', 'ë™ì•„ì¼ë³´': '020', 'ì¡°ì„ ì¼ë³´': '023', 'ì¤‘ì•™ì¼ë³´': '025'}
    75               headers = {'User-Agent': 'Mozilla/5.0'}
    76               all_articles = []
    77               for name, oid in newspapers.items():
    78                   try:
    79                       url = f"https://media.naver.com/press/{oid}/newspaper"
    80                       response = requests.get(url, headers=headers, verify=False, timeout=10)
    81                       response.raise_for_status()
    82                       soup = BeautifulSoup(response.text, 'html.parser')
    83                       links = soup.select('div.sc_offc_lst._paper_article_list a')
    84                       for link in links:
    85                           title = link.get_text(strip=True)
    86                           href = link.get('href', '')
    87                           if title and href:
    88                               all_articles.append({'ì‹ ë¬¸ì‚¬': name, 'ì œëª©': title, 'ë§í¬': 'https://media.naver.com' + href if href.startswith('/') else href})
    89                       time.sleep(0.5)
    90                   except Exception as e:
    91                       print(f'  âœ— {name} ìˆ˜ì§‘ ì˜¤ë¥˜: {e}')
    92               df = pd.DataFrame(all_articles).drop_duplicates(subset=['ë§í¬']).reset_index(drop=True)
    93               print(f'  - ì´ {len(df)}ê°œì˜ ê³ ìœ  ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘ ì™„ë£Œ.')
    94               return df
    95 
    96           def get_article_content(url):
    97               try:
    98                   headers = {'User-Agent': 'Mozilla/5.0'}
    99                   response = requests.get(url, headers=headers, timeout=10, verify=False)
   100                   response.raise_for_status()
   101                   soup = BeautifulSoup(response.text, 'html.parser')
   102                   content_area = soup.select_one('#dic_area, #articeBody, #article_content')
   103                   if content_area:
   104                       for el in content_area.select('script, style, .reporter_area, .ad_area, .promotion_area, div.byline, a, span.end_photo_org'):
   105                           el.decompose()
   106                       return content_area.get_text(strip=True)
   107               except Exception as e:
   108                   print(f'    - ë§í¬ í¬ë¡¤ë§ ì‹¤íŒ¨: {url}, ì˜¤ë¥˜: {e}')
   109               return ''
   110 
   111           def run_step_one_crawling_and_preprocessing():
   112               articles_df = get_today_articles()
   113               if articles_df.empty: return pd.DataFrame()
   114               print('--- 1.2: ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§ ë° ìš”ì•½ ìƒì„± ---')
   115               crawled_data = []
   116               for _, row in tqdm(articles_df.iterrows(), total=len(articles_df), desc='  - ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§'):
   117                   content = get_article_content(row['ë§í¬'])
   118                   if content:
   119                       crawled_data.append([row['ì‹ ë¬¸ì‚¬'], row['ì œëª©'], row['ë§í¬'], content, content[:400] + '...'])
   120                   time.sleep(0.5)
   121               if not crawled_data:
   122                   print('í¬ë¡¤ë§ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.')
   123                   return pd.DataFrame()
   124               df = pd.DataFrame(crawled_data, columns=['ì‹ ë¬¸ì‚¬', 'ì œëª©', 'ë§í¬', 'ë³¸ë¬¸', 'ë³¸ë¬¸_ìš”ì•½'])
   125               print('--- 1.3: ë‚´ìš© ê¸°ë°˜ ì¤‘ë³µ ê¸°ì‚¬ ì œê±° (ìœ ì‚¬ë„ 0.6 ê¸°ì¤€) ---')
   126               if not df.empty and 'ë³¸ë¬¸' in df.columns and not df['ë³¸ë¬¸'].isnull().all():
   127                   df['processed_text_for_dedup'] = df['ë³¸ë¬¸'].apply(preprocess)
   128                   vectorizer = TfidfVectorizer(min_df=1, ngram_range=(1, 2))
   129                   tfidf_matrix = vectorizer.fit_transform(df['processed_text_for_dedup'])
   130                   cosine_sim = cosine_similarity(tfidf_matrix)
   131                   df['ë³¸ë¬¸_ê¸¸ì´_dedup'] = df['ë³¸ë¬¸'].str.len()
   132                   df = df.sort_values(by='ë³¸ë¬¸_ê¸¸ì´_dedup', ascending=False).reset_index(drop=True)
   133                   tfidf_matrix_sorted = vectorizer.fit_transform(df['processed_text_for_dedup'])
   134                   cosine_sim_sorted = cosine_similarity(tfidf_matrix_sorted)
   135                   to_drop = set()
   136                   for i in range(len(cosine_sim_sorted)):
   137                       if i in to_drop: continue
   138                       for j in range(i + 1, len(cosine_sim_sorted)):
   139                           if j in to_drop: continue
   140                           if cosine_sim_sorted[i, j] > 0.6:
   141                               to_drop.add(j)
   142                   if to_drop:
   143                       print(f'   - {len(to_drop)}ê°œì˜ ìœ ì‚¬ ê¸°ì‚¬ë¥¼ ì œê±°í–ˆìŠµë‹ˆë‹¤.')
   144                       df.drop(index=list(to_drop), inplace=True)
   145                   df.drop(columns=['processed_text_for_dedup', 'ë³¸ë¬¸_ê¸¸ì´_dedup'], inplace=True)
   146               print(f'   - ìµœì¢… ë¶„ì„ ëŒ€ìƒ ê¸°ì‚¬: {len(df)}ê°œ')
   147               print('--- 1ë‹¨ê³„: í¬ë¡¤ë§ ë° ì „ì²˜ë¦¬ ì™„ë£Œ ---')
   148               print()
   149               return df
   150 
   151           def run_step_two_prediction(df):
   152               if df.empty:
   153                   print('2ë‹¨ê³„ ì‹¤íŒ¨: ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.')
   154                   return False
   155               print('--- 2.1: ëª¨ë¸ ë° Vectorizer ë¡œë“œ ---')
   156               try:
   157                   with open(MODEL_PATH, 'rb') as f: model = pickle.load(f)
   158                   with open(VECTORIZER_PATH, 'rb') as f: tfidf_vectorizer = pickle.load(f)
   159               except FileNotFoundError:
   160                   print(f'ì˜¤ë¥˜: ëª¨ë¸ íŒŒì¼({MODEL_PATH}) ë˜ëŠ” Vectorizer íŒŒì¼({VECTORIZER_PATH})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
   161                   return False
   162               print('--- 2.2: í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° íŠ¹ì„± ì¶”ì¶œ ---')
   163               keywords = {'ì—…ê³„': ['ì‹í’ˆ', 'í™”í•™', 'ë°”ì´ì˜¤', 'íŒ¨í‚¤ì§•', 'í”Œë¼ìŠ¤í‹±', 'í•­ì•”ì œ', 'ë°°í„°ë¦¬', 'ì¹œí™˜ê²½', 'Dë¨', 'ì‚¼ì–‘', 'ì´ˆìˆœìˆ˜', 'ì œì•½ì‚¬', 'ìˆ™ì·¨', 'ìƒì¾Œí™˜', 'ì„¤íƒ•', 'ì¹¼ë¡œë¦¬', 'ì‚¼ì–‘ê·¸ë£¹', 'ì‚¼ì–‘ì‚¬', 'ì‚¼ì–‘íŒ¨í‚¤ì§•', 'ì‚¼ì–‘ì—”ì”¨ì¼', 'ì‚¼ì–‘ë°”ì´ì˜¤íŒœ'], 'ê²½ì˜': ['ê²½ì˜', 'ê²½ì œ',
       'í™˜ìœ¨', 'M&A', 'ì¸ìˆ˜', 'íˆ¬ì', 'ì‹¤ì ', 'í•œì¼ê²½ì œí˜‘íšŒ', 'ì¡°ì§ë¬¸í™”', 'ë¬´ì—­']}
   164               df['processed_title'] = df['ì œëª©'].apply(preprocess)
   165               df['processed_text'] = df.apply(lambda r: preprocess(r['ì œëª©'] + ' ' + r['ë³¸ë¬¸']), axis=1)
   166               df['ì—…ê³„_í‚¤ì›Œë“œ_ê°œìˆ˜'] = df['processed_text'].apply(lambda x: sum(k in x for k in keywords['ì—…ê³„']))
   167               df['ê²½ì˜_í‚¤ì›Œë“œ_ê°œìˆ˜'] = df['processed_text'].apply(lambda x: sum(k in x for k in keywords['ê²½ì˜']))
   168               df['ì—…ê³„_í‚¤ì›Œë“œ_ì œëª©_ê°œìˆ˜'] = df['processed_title'].apply(lambda x: sum(k in x for k in keywords['ì—…ê³„']))
   169               df['ê²½ì˜_í‚¤ì›Œë“œ_ì œëª©_ê°œìˆ˜'] = df['processed_title'].apply(lambda x: sum(k in x for k in keywords['ê²½ì˜']))
   170               df['ë³¸ë¬¸_ê¸¸ì´'] = df['ë³¸ë¬¸'].str.len().replace(0, 1)
   171               df['ì—…ê³„_í‚¤ì›Œë“œ_ë°€ë„'] = df['ì—…ê³„_í‚¤ì›Œë“œ_ê°œìˆ˜'] / df['ë³¸ë¬¸_ê¸¸ì´']
   172               df['ê²½ì˜_í‚¤ì›Œë“œ_ë°€ë„'] = df['ê²½ì˜_í‚¤ì›Œë“œ_ê°œìˆ˜'] / df['ë³¸ë¬¸_ê¸¸ì´']
   173               text_features = tfidf_vectorizer.transform(df['processed_text'])
   174               metadata_features = df[['ì—…ê³„_í‚¤ì›Œë“œ_ê°œìˆ˜', 'ê²½ì˜_í‚¤ì›Œë“œ_ê°œìˆ˜', 'ì—…ê³„_í‚¤ì›Œë“œ_ì œëª©_ê°œìˆ˜', 'ê²½ì˜_í‚¤ì›Œë“œ_ì œëª©_ê°œìˆ˜', 'ë³¸ë¬¸_ê¸¸ì´', 'ì—…ê³„_í‚¤ì›Œë“œ_ë°€ë„', 'ê²½ì˜_í‚¤ì›Œë“œ_ë°€ë„']].values
   175               X_new = hstack([text_features, metadata_features * 0.5]).tocsr()
   176               print('--- 2.3: ìŠ¤í¬ë© ê°€ì¹˜ ì ìˆ˜ ì˜ˆì¸¡ ë° ê²°ê³¼ ì €ì¥ ---')
   177               df['ì˜ˆì¸¡ì ìˆ˜'] = model.predict_proba(X_new)[:, 1]
   178               df['ì¹´í…Œê³ ë¦¬'] = 'ê¸°íƒ€'
   179               df.loc[df['ì—…ê³„_í‚¤ì›Œë“œ_ê°œìˆ˜'] > 0, 'ì¹´í…Œê³ ë¦¬'] = 'ì—…ê³„'
   180               df.loc[(df['ì—…ê³„_í‚¤ì›Œë“œ_ê°œìˆ˜'] == 0) & (df['ê²½ì˜_í‚¤ì›Œë“œ_ê°œìˆ˜'] > 0), 'ì¹´í…Œê³ ë¦¬'] = 'ê²½ì˜'
   181               df['ìµœì¢…ì„ íƒì—¬ë¶€'] = ''
   182               sorted_df = df.sort_values(by='ì˜ˆì¸¡ì ìˆ˜', ascending=False)
   183               output_columns = ['ì‹ ë¬¸ì‚¬', 'ì œëª©', 'ë³¸ë¬¸_ìš”ì•½', 'ë§í¬', 'ì¹´í…Œê³ ë¦¬', 'ì˜ˆì¸¡ì ìˆ˜', 'ìµœì¢…ì„ íƒì—¬ë¶€']
   184               final_df = sorted_df[output_columns]
   185               final_df.to_csv(SCRAPPED_NEWS_TODAY_CSV, index=False, encoding='utf-8-sig')
   186               print(f'  - ìµœì¢… ê²°ê³¼ê°€ \\'{SCRAPPED_NEWS_TODAY_CSV}\\' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.')
   187               print('--- 2ë‹¨ê³„: ìŠ¤í¬ë© ê°€ì¹˜ ì˜ˆì¸¡ ì™„ë£Œ ---')
   188               print()
   189               return True
   190 
   191           def generate_static_files():
   192               print('--- 3ë‹¨ê³„: GitHub Pagesìš© ì •ì  íŒŒì¼ ìƒì„± ì‹œì‘ ---')
   193               if not os.path.exists(SCRAPPED_NEWS_TODAY_CSV) or os.path.getsize(SCRAPPED_NEWS_TODAY_CSV) == 0:
   194                   print(f'  ì˜¤ë¥˜: \\'{SCRAPPED_NEWS_TODAY_CSV}\\' íŒŒì¼ì´ ë¹„ì–´ìˆê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.')
   195                   return False
   196               try:
   197                   print('  [1/3] JSON íŒŒì¼ ìƒì„± ì¤‘...')
   198                   df = pd.read_csv(SCRAPPED_NEWS_TODAY_CSV, encoding='utf-8-sig')
   199                   df.to_json(JSON_OUTPUT_FILE, orient='records', force_ascii=False, indent=4)
   200                   print(f'    \\'{JSON_OUTPUT_FILE}\\' ìƒì„± ì™„ë£Œ.')
   201               except Exception as e:
   202                   print(f'  ì˜¤ë¥˜: JSON ë³€í™˜ ì‹¤íŒ¨ - {e}')
   203                   return False
   204               print('  [2/3] HTML íŒŒì¼ ì²˜ë¦¬ ê±´ë„ˆëœ€ (index.htmlì´ ì´ë¯¸ ì˜¬ë°”ë¥¸ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì°¸ì¡°).')
   205               try:
   206                   print('  [3/3] JavaScript íŒŒì¼ ìƒì„± ì¤‘...')
   207                   os.makedirs(os.path.dirname(JS_OUTPUT_FILE), exist_ok=True)
   208                   with open(JS_TEMPLATE_FILE, 'r', encoding='utf-8') as f: js_content = f.read()
   209                   modified_js = js_content.replace("fetch('/api/news')", "fetch('news_data.json')")
   210                   modified_js = modified_js.replace("setInterval(fetchNews, 30000);", '/* ìë™ ìƒˆë¡œê³ ì¹¨ ë¹„í™œì„±í™” */')
   211                   with open(JS_OUTPUT_FILE, 'w', encoding='utf-8') as f: f.write(modified_js)
   212                   print(f'    \\'{JS_OUTPUT_FILE}\\' ìƒì„± ì™„ë£Œ.')
   213               except FileNotFoundError:
   214                   print(f'  ì˜¤ë¥˜: JS í…œí”Œë¦¿ íŒŒì¼ \\'{JS_TEMPLATE_FILE}\\'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
   215                   return False
   216               except Exception as e:
   217                   print(f'  ì˜¤ë¥˜: JavaScript íŒŒì¼ ìƒì„± ì‹¤íŒ¨ - {e}')
   218                   return False
   219               print('--- 3ë‹¨ê³„: ì •ì  íŒŒì¼ ìƒì„± ì™„ë£Œ ---')
   220               print()
   221               return True
   222 
   223           # --- ë©”ì¸ ì‹¤í–‰ ë¸”ë¡ ---
   224           if __name__ == '__main__':
   225               processed_articles_df = run_step_one_crawling_and_preprocessing()
   226               if not processed_articles_df.empty:
   227                   prediction_success = run_step_two_prediction(processed_articles_df)
   228                   if prediction_success:
   229                       if generate_static_files():
   230                           print('ğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.')
   231                       else:
   232                           print('!!!!! 3ë‹¨ê³„(ì •ì  íŒŒì¼ ìƒì„±)ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. !!!!!')
   233                   else:
   234                       print('!!!!! 2ë‹¨ê³„(ìŠ¤í¬ë© ì˜ˆì¸¡)ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. !!!!!')
   235               else:
   236                   print('!!!!! 1ë‹¨ê³„(í¬ë¡¤ë§ ë° ì „ì²˜ë¦¬)ì—ì„œ ì²˜ë¦¬í•  ê¸°ì‚¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. !!!!!')
   237           """
   238 
   239       - name: Commit and push if it changed
   240         uses: stefanzweifel/git-auto-commit-action@v5
   241         with:
   242           commit_message: 'chore: Daily news data updated'
   243           file_pattern: 'news_data.json index.html static/js/github_pages_main.js scrapped_news_today.csv'
