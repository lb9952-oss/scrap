     1 name: Daily News Update
     2 on:
     3   schedule:
     4     - cron: '0 21 * * *'
     5   workflow_dispatch:
     6 jobs:
     7   build-and-commit:
     8     runs-on: ubuntu-latest
     9     permissions:
    10       contents: write
    11     steps:
    12       - name: Checkout repository
    13         uses: actions/checkout@v4
    14       - name: Set up JDK 17
    15         uses: actions/setup-java@v4
    16         with:
    17           java-version: '17'
    18           distribution: 'temurin'
    19       - name: Set up Python
    20         uses: actions/setup-python@v5       
    21         with:
    22           python-version: '3.9'
    23       - name: Install Python dependencies   
    24         run: pip install -r requirements.txt
    25       - name: Run Python Script
    26         run: |
    27           python -c """
    28           import pandas as pd
    29           import numpy as np
    30           import re
    31           import pickle
    32           import os
    33           import requests
    34           from bs4 import BeautifulSoup
    35           from konlpy.tag import Okt
    36           from sklearn.feature_extraction.text import TfidfVectorizer
    37           from scipy.sparse import hstack
    38           import time
    39           import json
    40           from tqdm import tqdm
    41           from sklearn.metrics.pairwise import cosine_similarity
    42 
    43           os.environ['CURL_CA_BUNDLE'] = ''
    44           os.environ['REQUESTS_CA_BUNDLE'] = ''
    45           os.environ['HF_HUB_DISABLE_CERTIFICATE_VERIFICATION'] = '1'
    46 
    47           BASE_DIR = os.getcwd()
    48           SCRAPPED_NEWS_TODAY_CSV = os.path.join(BASE_DIR, 'scrapped_news_today.csv')
    49           MODEL_PATH = os.path.join(BASE_DIR, 'scrap_model.pkl')
    50           VECTORIZER_PATH = os.path.join(BASE_DIR, 'tfidf_vectorizer.pkl')
    51           JSON_OUTPUT_FILE = os.path.join(BASE_DIR, 'news_data.json')
    52           HTML_TEMPLATE_FILE = os.path.join(BASE_DIR, 'index.html')
    53           HTML_OUTPUT_FILE = os.path.join(BASE_DIR, 'index.html')
    54           JS_TEMPLATE_FILE = os.path.join(BASE_DIR, 'static', 'js', 'main.js')
    55           JS_OUTPUT_FILE = os.path.join(BASE_DIR, 'static', 'js', 'github_pages_main.js')
    56 
    57           okt = Okt()
    58           def preprocess(text):
    59               return ' '.join(okt.nouns(re.sub(r'[^\ã„±-ã…ã…-ã…£ê°€-í£ ]','', str(text))))
    60 
    61           def get_today_articles():
    62               print('--- 1.1: ì£¼ìš” ì¼ê°„ì§€ ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘ ì‹œì‘ ---')
    63               newspapers = {'í•œêµ­ê²½ì œ': '015', 'ë§¤ì¼ê²½ì œ': '009', 'ë™ì•„ì¼ë³´': '020', 'ì¡°ì„ ì¼ë³´': '023', 'ì¤‘ì•™ì¼ë³´': '025'}
    64               headers = {'User-Agent': 'Mozilla/5.0'}
    65               all_articles = []
    66               for name, oid in newspapers.items():
    67                   try:
    68                       url = f"https://media.naver.com/press/{oid}/newspaper"
    69                       response = requests.get(url, headers=headers, verify=False, timeout=10)
    70                       response.raise_for_status()
    71                       soup = BeautifulSoup(response.text, 'html.parser')
    72                       links = soup.select('div.sc_offc_lst._paper_article_list a')
    73                       for link in links:
    74                           title = link.get_text(strip=True)
    75                           href = link.get('href', '')
    76                           if title and href:
    77                               all_articles.append({'ì‹ ë¬¸ì‚¬': name, 'ì œëª©': title, 'ë§í¬': 'https://media.naver.com' + href if href.startswith('/') else href})
    78                       time.sleep(0.5)
    79                   except Exception as e:
    80                       print(f'  - {name} ìˆ˜ì§‘ ì˜¤ë¥˜: {e}')
    81               df = pd.DataFrame(all_articles).drop_duplicates(subset=['ë§í¬']).reset_index(drop=True)
    82               print(f'  - ì´ {len(df)}ê°œì˜ ê³ ìœ  ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘ ì™„ë£Œ.')
    83               return df
    84 
    85           def get_article_content(url):
    86               try:
    87                   headers = {'User-Agent': 'Mozilla/5.0'}
    88                   response = requests.get(url, headers=headers, timeout=10, verify=False)
    89                   response.raise_for_status()
    90                   soup = BeautifulSoup(response.text, 'html.parser')
    91                   content_area = soup.select_one('#dic_area, #articeBody, #article_content')
    92                   if content_area:
    93                       for el in content_area.select('script, style, .reporter_area, .ad_area, .promotion_area, div.byline, a, span.end_photo_org'):
    94                           el.decompose()
    95                       return content_area.get_text(strip=True)
    96               except Exception as e:
    97                   print(f'    - ë§í¬ í¬ë¡¤ë§ ì‹¤íŒ¨: {url}, ì˜¤ë¥˜: {e}')
    98               return ''
    99 
   100           def run_step_one_crawling_and_preprocessing():
   101               articles_df = get_today_articles()
   102               if articles_df.empty: return pd.DataFrame()
   103               print('--- 1.2: ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§ ë° ìš”ì•½ ìƒì„± ---')
   104               crawled_data = []
   105               for _, row in tqdm(articles_df.iterrows(), total=len(articles_df), desc='  - ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§'):
   106                   content = get_article_content(row['ë§í¬'])
   107                   if content:
   108                       crawled_data.append([row['ì‹ ë¬¸ì‚¬'], row['ì œëª©'], row['ë§í¬'], content, content[:400] + '...'])
   109                   time.sleep(0.5)
   110               if not crawled_data:
   111                   print('í¬ë¡¤ë§ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.')
   112                   return pd.DataFrame()
   113               df = pd.DataFrame(crawled_data, columns=['ì‹ ë¬¸ì‚¬', 'ì œëª©', 'ë§í¬', 'ë³¸ë¬¸', 'ë³¸ë¬¸_ìš”ì•½'])
   114               print('--- 1.3: ë‚´ìš© ê¸°ë°˜ ì¤‘ë³µ ê¸°ì‚¬ ì œê±° (ìœ ì‚¬ë„ 0.6 ê¸°ì¤€) ---')
   115               if not df.empty and 'ë³¸ë¬¸' in df.columns and not df['ë³¸ë¬¸'].isnull().all():
   116                   df['processed_text_for_dedup'] = df['ë³¸ë¬¸'].apply(preprocess)
   117                   vectorizer = TfidfVectorizer(min_df=1, ngram_range=(1, 2))
   118                   tfidf_matrix = vectorizer.fit_transform(df['processed_text_for_dedup'])
   119                   cosine_sim = cosine_similarity(tfidf_matrix)
   120                   df['ë³¸ë¬¸_ê¸¸ì´_dedup'] = df['ë³¸ë¬¸'].str.len()
   121                   df = df.sort_values(by='ë³¸ë¬¸_ê¸¸ì´_dedup', ascending=False).reset_index(drop=True)
   122                   tfidf_matrix_sorted = vectorizer.fit_transform(df['processed_text_for_dedup'])
   123                   cosine_sim_sorted = cosine_similarity(tfidf_matrix_sorted)
   124                   to_drop = set()
   125                   for i in range(len(cosine_sim_sorted)):
   126                       if i in to_drop: continue
   127                       for j in range(i + 1, len(cosine_sim_sorted)):
   128                           if j in to_drop: continue
   129                           if cosine_sim_sorted[i, j] > 0.6:
   130                               to_drop.add(j)
   131                   if to_drop:
   132                       print(f'   - {len(to_drop)}ê°œì˜ ìœ ì‚¬ ê¸°ì‚¬ë¥¼ ì œê±°í–ˆìŠµë‹ˆë‹¤.')
   133                       df.drop(index=list(to_drop), inplace=True)
   134                   df.drop(columns=['processed_text_for_dedup', 'ë³¸ë¬¸_ê¸¸ì´_dedup'], inplace=True)
   135               print(f'   - ìµœì¢… ë¶„ì„ ëŒ€ìƒ ê¸°ì‚¬: {len(df)}ê°œ')
   136               print('--- 1ë‹¨ê³„: í¬ë¡¤ë§ ë° ì „ì²˜ë¦¬ ì™„ë£Œ ---')
   137               print()
   138               return df
   139 
   140           def run_step_two_prediction(df):
   141               if df.empty:
   142                   print('2ë‹¨ê³„ ì‹¤íŒ¨: ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.')
   143                   return False
   144               print('--- 2.1: ëª¨ë¸ ë° Vectorizer ë¡œë“œ ---')
   145               try:
   146                   with open(MODEL_PATH, 'rb') as f: model = pickle.load(f)
   147                   with open(VECTORIZER_PATH, 'rb') as f: tfidf_vectorizer = pickle.load(f)
   148               except FileNotFoundError:
   149                   print(f'ì˜¤ë¥˜: ëª¨ë¸ íŒŒì¼({MODEL_PATH}) ë˜ëŠ” Vectorizer íŒŒì¼({VECTORIZER_PATH})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
   150                   return False
   151               print('--- 2.2: í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° íŠ¹ì„± ì¶”ì¶œ ---')
   152               keywords = {'ì—…ê³„': ['ì‹í’ˆ', 'í™”í•™', 'ë°”ì´ì˜¤', 'íŒ¨í‚¤ì§•', 'í”Œë¼ìŠ¤í‹±', 'í•­ì•”ì œ', 'ë°°í„°ë¦¬', 'ì¹œí™˜ê²½', 'Dë¨', 'ì‚¼ì–‘', 'ì´ˆìˆœìˆ˜', 'ì œì•½ì‚¬', 'ìˆ™ì·¨', 'ìƒì¾Œí™˜', 'ì„¤íƒ•', 'ì¹¼ë¡œë¦¬', 'ì‚¼ì–‘ê·¸ë£¹', 'ì‚¼ì–‘ì‚¬', 'ì‚¼ì–‘íŒ¨í‚¤ì§•', 'ì‚¼ì–‘ì—”ì”¨ì¼', 'ì‚¼ì–‘ë°”ì´ì˜¤íŒœ'], 'ê²½ì˜': ['ê²½ì˜', 'ê²½ì œ',
       'í™˜ìœ¨', 'M&A', 'ì¸ìˆ˜', 'íˆ¬ì', 'ì‹¤ì ', 'í•œì¼ê²½ì œí˜‘íšŒ', 'ì¡°ì§ë¬¸í™”', 'ë¬´ì—­']}
   153               df['processed_title'] = df['ì œëª©'].apply(preprocess)
   154               df['processed_text'] = df.apply(lambda r: preprocess(r['ì œëª©'] + ' ' + r['ë³¸ë¬¸']), axis=1)
   155               df['ì—…ê³„_í‚¤ì›Œë“œ_ê°œìˆ˜'] = df['processed_text'].apply(lambda x: sum(k in x for k in keywords['ì—…ê³„']))
   156               df['ê²½ì˜_í‚¤ì›Œë“œ_ê°œìˆ˜'] = df['processed_text'].apply(lambda x: sum(k in x for k in keywords['ê²½ì˜']))
   157               df['ì—…ê³„_í‚¤ì›Œë“œ_ì œëª©_ê°œìˆ˜'] = df['processed_title'].apply(lambda x: sum(k in x for k in keywords['ì—…ê³„']))
   158               df['ê²½ì˜_í‚¤ì›Œë“œ_ì œëª©_ê°œìˆ˜'] = df['processed_title'].apply(lambda x: sum(k in x for k in keywords['ê²½ì˜']))
   159               df['ë³¸ë¬¸_ê¸¸ì´'] = df['ë³¸ë¬¸'].str.len().replace(0, 1)
   160               df['ì—…ê³„_í‚¤ì›Œë“œ_ë°€ë„'] = df['ì—…ê³„_í‚¤ì›Œë“œ_ê°œìˆ˜'] / df['ë³¸ë¬¸_ê¸¸ì´']
   161               df['ê²½ì˜_í‚¤ì›Œë“œ_ë°€ë„'] = df['ê²½ì˜_í‚¤ì›Œë“œ_ê°œìˆ˜'] / df['ë³¸ë¬¸_ê¸¸ì´']
   162               text_features = tfidf_vectorizer.transform(df['processed_text'])
   163               metadata_features = df[['ì—…ê³„_í‚¤ì›Œë“œ_ê°œìˆ˜', 'ê²½ì˜_í‚¤ì›Œë“œ_ê°œìˆ˜', 'ì—…ê³„_í‚¤ì›Œë“œ_ì œëª©_ê°œìˆ˜', 'ê²½ì˜_í‚¤ì›Œë“œ_ì œëª©_ê°œìˆ˜', 'ë³¸ë¬¸_ê¸¸ì´', 'ì—…ê³„_í‚¤ì›Œë“œ_ë°€ë„', 'ê²½ì˜_í‚¤ì›Œë“œ_ë°€ë„']].values
   164               X_new = hstack([text_features, metadata_features * 0.5]).tocsr()
   165               print('--- 2.3: ìŠ¤í¬ë© ê°€ì¹˜ ì ìˆ˜ ì˜ˆì¸¡ ë° ê²°ê³¼ ì €ì¥ ---')
   166               df['ì˜ˆì¸¡ì ìˆ˜'] = model.predict_proba(X_new)[:, 1]
   167               df['ì¹´í…Œê³ ë¦¬'] = 'ê¸°íƒ€'
   168               df.loc[df['ì—…ê³„_í‚¤ì›Œë“œ_ê°œìˆ˜'] > 0, 'ì¹´í…Œê³ ë¦¬'] = 'ì—…ê³„'
   169               df.loc[(df['ì—…ê³„_í‚¤ì›Œë“œ_ê°œìˆ˜'] == 0) & (df['ê²½ì˜_í‚¤ì›Œë“œ_ê°œìˆ˜'] > 0), 'ì¹´í…Œê³ ë¦¬'] = 'ê²½ì˜'
   170               df['ìµœì¢…ì„ íƒì—¬ë¶€'] = ''
   171               sorted_df = df.sort_values(by='ì˜ˆì¸¡ì ìˆ˜', ascending=False)
   172               output_columns = ['ì‹ ë¬¸ì‚¬', 'ì œëª©', 'ë³¸ë¬¸_ìš”ì•½', 'ë§í¬', 'ì¹´í…Œê³ ë¦¬', 'ì˜ˆì¸¡ì ìˆ˜', 'ìµœì¢…ì„ íƒì—¬ë¶€']
   173               final_df = sorted_df[output_columns]
   174               final_df.to_csv(SCRAPPED_NEWS_TODAY_CSV, index=False, encoding='utf-8-sig')
   175               print(f'  - ìµœì¢… ê²°ê³¼ê°€ {SCRAPPED_NEWS_TODAY_CSV} íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.')
   176               print('--- 2ë‹¨ê³„: ìŠ¤í¬ë© ê°€ì¹˜ ì˜ˆì¸¡ ì™„ë£Œ ---')
   177               print()
   178               return True
   179 
   180           def generate_static_files():
   181               print('--- 3ë‹¨ê³„: GitHub Pagesìš© ì •ì  íŒŒì¼ ìƒì„± ì‹œì‘ ---')
   182               if not os.path.exists(SCRAPPED_NEWS_TODAY_CSV) or os.path.getsize(SCRAPPED_NEWS_TODAY_CSV) == 0:
   183                   print(f'  ì˜¤ë¥˜: {SCRAPPED_NEWS_TODAY_CSV} íŒŒì¼ì´ ë¹„ì–´ìˆê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.')
   184                   return False
   185               try:
   186                   print('  [1/3] JSON íŒŒì¼ ìƒì„± ì¤‘...')
   187                   df = pd.read_csv(SCRAPPED_NEWS_TODAY_CSV, encoding='utf-8-sig')
   188                   df.to_json(JSON_OUTPUT_FILE, orient='records', force_ascii=False, indent=4)
   189                   print(f'    {JSON_OUTPUT_FILE} ìƒì„± ì™„ë£Œ.')
   190               except Exception as e:
   191                   print(f'  ì˜¤ë¥˜: JSON ë³€í™˜ ì‹¤íŒ¨ - {e}')
   192                   return False
   193               print('  [2/3] HTML íŒŒì¼ ì²˜ë¦¬ ê±´ë„ˆëœ€ (index.htmlì´ ì´ë¯¸ ì˜¬ë°”ë¥¸ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì°¸ì¡°).')
   194               try:
   195                   print('  [3/3] JavaScript íŒŒì¼ ìƒì„± ì¤‘...')
   196                   os.makedirs(os.path.dirname(JS_OUTPUT_FILE), exist_ok=True)
   197                   with open(JS_TEMPLATE_FILE, 'r', encoding='utf-8') as f: js_content = f.read()
   198                   modified_js = js_content.replace("fetch('/api/news')", "fetch('news_data.json')")
   199                   modified_js = modified_js.replace("setInterval(fetchNews, 30000);", '/* ìë™ ìƒˆë¡œê³ ì¹¨ ë¹„í™œì„±í™” */')
   200                   with open(JS_OUTPUT_FILE, 'w', encoding='utf-8') as f: f.write(modified_js)
   201                   print(f'    {JS_OUTPUT_FILE} ìƒì„± ì™„ë£Œ.')
   202               except FileNotFoundError:
   203                   print(f'  ì˜¤ë¥˜: JS í…œí”Œë¦¿ íŒŒì¼ {JS_TEMPLATE_FILE}ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
   204                   return False
   205               except Exception as e:
   206                   print(f'  ì˜¤ë¥˜: JavaScript íŒŒì¼ ìƒì„± ì‹¤íŒ¨ - {e}')
   207                   return False
   208               print('--- 3ë‹¨ê³„: ì •ì  íŒŒì¼ ìƒì„± ì™„ë£Œ ---')
   209               print()
   210               return True
   211 
   212           if __name__ == '__main__':
   213               processed_articles_df = run_step_one_crawling_and_preprocessing()
   214               if not processed_articles_df.empty:
   215                   prediction_success = run_step_two_prediction(processed_articles_df)
   216                   if prediction_success:
   217                       if generate_static_files():
   218                           print('ğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.')
   219                       else:
   220                           print('!!!!! 3ë‹¨ê³„(ì •ì  íŒŒì¼ ìƒì„±)ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. !!!!!')
   221                   else:
   222                       print('!!!!! 2ë‹¨ê³„(ìŠ¤í¬ë© ì˜ˆì¸¡)ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. !!!!!')
   223               else:
   224                   print('!!!!! 1ë‹¨ê³„(í¬ë¡¤ë§ ë° ì „ì²˜ë¦¬)ì—ì„œ ì²˜ë¦¬í•  ê¸°ì‚¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. !!!!!')
   225           """
   226 
   227       - name: Commit and push if it changed
   228         uses: stefanzweifel/git-auto-commit-action@v5
   229         with:
   230           commit_message: 'chore: Daily news data updated'
   231           file_pattern: 'news_data.json index.html static/js/github_pages_main.js scrapped_news_today.csv'
